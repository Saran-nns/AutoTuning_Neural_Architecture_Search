{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "incremental_learning_model_wrapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVvGRP01bLayveyO1+aBNx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saran-nns/incremental_learning_tf2.0/blob/master/incremental_learning_model_wrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G20jqN06tTXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX-Cv5Wwts3F",
        "colab_type": "text"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Z3Vxc9tanf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "eab26b9a-a1df-4c66-b54d-ca32106c4832"
      },
      "source": [
        "dataset = datasets.cifar10.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DKjQek3tNJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_dataset_loader(dataset = dataset, new_data = None, old_labels=[0,1,2,3,4,5,6,7], new_labels= [8,9]):\n",
        "  \n",
        "  filtered_training_data = []\n",
        "  filtered_labels = []\n",
        "  allowed_labels = old_labels + new_labels\n",
        "  \n",
        "  # Pick only the data with required labels\n",
        "  for i, _ in enumerate(dataset[0][0]):\n",
        "    if dataset[0][1][i] in allowed_labels:\n",
        "      filtered_training_data.append(dataset[0][0][i]/np.float32(255))\n",
        "      filtered_labels.append(dataset[0][1][i])\n",
        "\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((filtered_training_data, filtered_labels))\n",
        "  # TODO: new_dataset = train_dataset+old_dataset\n",
        "  train_dataset.shuffle(len(list(train_dataset)))\n",
        "\n",
        "  # Split into train and test (80%, 20%)\n",
        "  trainind_dataset_size = int(len(list(train_dataset))*0.8)\n",
        "  train_ds,test_ds = train_dataset.take(trainind_dataset_size), train_dataset.skip(trainind_dataset_size)\n",
        "\n",
        "  # Split the images and labels in train and test datasets\n",
        "  train_images,train_labels= np.array(list(train_ds))[:,0],np.array(list(train_ds))[:,1]\n",
        "  train_images,train_labels = tf.convert_to_tensor(train_images.tolist()), tf.convert_to_tensor(train_labels.tolist())\n",
        "  test_images,test_labels = np.array(list(test_ds))[:,0],np.array(list(test_ds))[:,1]\n",
        "  test_images,test_labels = tf.convert_to_tensor(test_images.tolist()),tf.convert_to_tensor(test_labels.tolist())\n",
        "  print('Shape of training images after filtering labels',np.shape(train_images))\n",
        "\n",
        "  return train_images,train_labels,test_images,test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlJXDwWJtvVk",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8SbDsMWAq0P",
        "colab_type": "text"
      },
      "source": [
        "##### Conditions for adding new layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGNDyg7ltg13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeatureExtractor(Model):\n",
        "  def __init__(self,trainable):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    self.conv1 = Conv2D(32, 3, activation='relu',trainable = trainable)\n",
        "    self.maxpool = MaxPooling2D((2,2))\n",
        "    self.conv2 = Conv2D(64, 3, activation='relu',trainable = trainable)\n",
        "    self.conv3 = Conv2D(64,3,activation='relu',trainable = trainable)\n",
        "  def call(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.conv3(x)\n",
        "    return x\n",
        "\n",
        "class Classifier(Model):\n",
        "  def __init__(self, n_classes):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(64, activation='relu')\n",
        "    self.d2 = Dense(n_classes)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    return self.d2(x)\n",
        "\n",
        "class IncrementalLearning(Model):\n",
        "  def __init__(self, feature_extractor,classifier = None, is_online = False, add_new_prediction_layer = False):\n",
        "    super(IncrementalLearning, self).__init__()\n",
        "    self.feature_extractor=feature_extractor\n",
        "    self.classifier = classifier\n",
        "    if is_online:\n",
        "      self.feature_extractor.trainable = False\n",
        "\n",
        "    if classifier is not None:\n",
        "      # Get the layers of the classifier\n",
        "      layer_names = [var.name for var in classifier.trainable_variables]\n",
        "      if not layer_names:\n",
        "\n",
        "      if add_new_prediction_layer:\n",
        "        self.classifier.trainable = False\n",
        "        # Create new layer inside classifier model\n",
        "        \n",
        "    \n",
        "\n",
        "  def call(self, x):\n",
        "    features = self.feature_extractor(x)\n",
        "    x = self.classifier(features)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chhYAHUb1QU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "d7c6710d-d935-4d79-b69d-cf76cd971606"
      },
      "source": [
        "x = Classifier(10)\n",
        "x.build((64,))\n",
        "var = [var.name for var in x.trainable_variables]\n",
        "var"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dense_20/kernel:0',\n",
              " 'dense_20/bias:0',\n",
              " 'dense_21/kernel:0',\n",
              " 'dense_21/bias:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QIkFmChwBvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = IncrementalLearning(feature_extractor,classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}