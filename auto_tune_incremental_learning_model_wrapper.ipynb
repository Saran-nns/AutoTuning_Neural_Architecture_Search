{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Tutorial: Introduction to \n",
    "# Incremental Learning + Auto-tuning during Neural Architecture Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements:\n",
    "\n",
    "GPU(s) with CUDA support\n",
    "\n",
    "Python >= 3.6\n",
    "\n",
    "Tensorflow = 2.3\n",
    "\n",
    "CUDA = 10.2\n",
    "\n",
    "CuDNN = 7.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G20jqN06tTXm"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set GPU memory usage limit\n",
    "##### Try/experiment with respect to the complexity of the model and size of the batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 4 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 4)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KX-Cv5Wwts3F"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "E6Z3Vxc9tanf",
    "outputId": "d8040fc3-703b-4529-8a5c-41d26b82147a"
   },
   "outputs": [],
   "source": [
    "dataset = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper function to slice/split the dataset with target labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DKjQek3tNJf"
   },
   "outputs": [],
   "source": [
    "def custom_dataset_loader(dataset = dataset, allowed_labels =None):\n",
    "  \n",
    "    filtered_training_data = []\n",
    "    filtered_labels = []\n",
    "    # Pick only the data with required labels\n",
    "    for i, _ in enumerate(dataset[0][0]):\n",
    "        if dataset[0][1][i] in allowed_labels:\n",
    "            filtered_training_data.append(dataset[0][0][i]/np.float32(255))\n",
    "            filtered_labels.append(dataset[0][1][i])\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((filtered_training_data, filtered_labels))\n",
    "    # TODO: new_dataset = train_dataset+old_dataset\n",
    "    train_dataset.shuffle(len(list(train_dataset)))\n",
    "    \n",
    "    # Split into train and test (80%, 20%)\n",
    "    trainind_dataset_size = int(len(list(train_dataset))*0.8)\n",
    "    train_ds,test_ds = train_dataset.take(trainind_dataset_size), train_dataset.skip(trainind_dataset_size)\n",
    "\n",
    "    # Split the images and labels in train and test datasets\n",
    "    train_images,train_labels= np.array(list(train_ds))[:,0],np.array(list(train_ds))[:,1]\n",
    "    train_images,train_labels = tf.convert_to_tensor(train_images.tolist()), tf.convert_to_tensor(train_labels.tolist())\n",
    "    test_images,test_labels = np.array(list(test_ds))[:,0],np.array(list(test_ds))[:,1]\n",
    "    test_images,test_labels = tf.convert_to_tensor(test_images.tolist()),tf.convert_to_tensor(test_labels.tolist())\n",
    "    print('Shape of training images after filtering labels',np.shape(train_images))\n",
    "\n",
    "    return train_images,train_labels,test_images,test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlJXDwWJtvVk"
   },
   "source": [
    "### Custom Feature extractor and Classifier Models\n",
    "\n",
    "#### CNN has two basic parts: \n",
    "Feature extractors with one or more convolutional layers,\n",
    "\n",
    "Classifier, which is usually single or multi fully connected layers.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TGNDyg7ltg13"
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(Model):\n",
    "    def __init__(self,trainable):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu',trainable = trainable)\n",
    "        self.maxpool = MaxPooling2D((2,2))\n",
    "        self.conv2 = Conv2D(64, 3, activation='relu',trainable = trainable)\n",
    "        self.conv3 = Conv2D(64,3,activation='relu',trainable = trainable)\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "class Classifier(Model):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.n_classes = n_classes\n",
    "        self.d1 = Dense(64, activation='relu')\n",
    "        self.d2 = Dense(n_classes)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and test the model wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Classifier(10)\n",
    "x.build((64,)) # Build using the input size\n",
    "var= [var.name for var in x.trainable_variables]\n",
    "var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Model: Wrapper for both Feature extractor and Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleModel(Model):\n",
    "    def __init__(self, feature_extractor,classifier, is_online = False):\n",
    "        super(SampleModel, self).__init__()\n",
    "        self.feature_extractor=feature_extractor\n",
    "        if is_online:\n",
    "            self.feature_extractor.trainable = False\n",
    "            # Clone the old classifier weights and  set it non-trainable\n",
    "            classifier.trainable = False\n",
    "            # Add new prediction layer and set it trainable\n",
    "            self.new_prediction_layer = tf.keras.layers.Dense(classifier.n_classes,trainable=True)\n",
    "            self.classifier= tf.keras.Sequential([classifier, self.new_prediction_layer])\n",
    "        else:\n",
    "            self.classifier = classifier\n",
    "\n",
    "    def call(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        x = self.classifier(features)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instantiate classifier and the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "fhAFuSUEuv-6",
    "outputId": "2a73fd19-8ff0-42d8-e5fe-e4f8caf61e45"
   },
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor(trainable=True)\n",
    "classifier = Classifier(10)\n",
    "model = SampleModel(feature_extractor,classifier,is_online=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "3d_4WO01tLcu",
    "outputId": "db9dc065-1362-4456-c69f-a50586111fdb"
   },
   "outputs": [],
   "source": [
    "model.build((None,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "_QIkFmChwBvl",
    "outputId": "9516a5d5-e17a-4342-a514-7196116fa85c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sample_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "feature_extractor (FeatureEx multiple                  56320     \n",
      "_________________________________________________________________\n",
      "classifier_1 (Classifier)    multiple                  66250     \n",
      "=================================================================\n",
      "Total params: 122,570\n",
      "Trainable params: 122,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "QVAiQcT7okO7",
    "outputId": "6e3fab48-00e0-4978-82bf-a592010de766"
   },
   "outputs": [],
   "source": [
    "feature_extractor1 = FeatureExtractor(trainable=False)\n",
    "classifier1 = Classifier(10)\n",
    "model2 = SampleModel(feature_extractor,classifier,is_online=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "GviDAEPaCrbw",
    "outputId": "fccc0884-b5d5-4daa-bcf8-5e5cd01d4a78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sample_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "feature_extractor (FeatureEx multiple                  56320     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 10)                66360     \n",
      "=================================================================\n",
      "Total params: 122,680\n",
      "Trainable params: 110\n",
      "Non-trainable params: 122,570\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.build((None,32,32,3))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qD48VEWFG8Rt"
   },
   "source": [
    "### Training Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AR9nzM84HCbF"
   },
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "aPSpy-r7G_B5",
    "outputId": "250b49ec-c4fe-4144-f80b-1f0a05cf369f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images after filtering labels (32000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "old_labels = [0,1,2,3,4,5,6,7]\n",
    "new_labels = []\n",
    "allowed_labels = old_labels + new_labels\n",
    "train_images, train_labels, test_images,test_labels = custom_dataset_loader(dataset = dataset, \n",
    "                                                                          allowed_labels = allowed_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKWC5Ivcqfci"
   },
   "source": [
    "#### Checkpoints and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmVvgishs6m3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U726EwG1pQgA"
   },
   "source": [
    "#### Train using Mirror strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "U3qlrRR6pZVt",
    "outputId": "efca6a42-2b94-4cb4-b6f9-02a1ce803a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Epoch 1/10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      " 983/1000 [============================>.] - ETA: 0s - loss: 2.0784 - acc: 0.1466INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "\n",
      "Epoch 00001: saving model to training_1\\cp.ckpt\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 2.0784 - acc: 0.1467 - val_loss: 2.0755 - val_acc: 0.1602\n",
      "Epoch 2/10\n",
      " 996/1000 [============================>.] - ETA: 0s - loss: 2.0730 - acc: 0.1719\n",
      "Epoch 00002: saving model to training_1\\cp.ckpt\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.0730 - acc: 0.1719 - val_loss: 2.0706 - val_acc: 0.1749\n",
      "Epoch 3/10\n",
      " 994/1000 [============================>.] - ETA: 0s - loss: 2.0685 - acc: 0.1892\n",
      "Epoch 00003: saving model to training_1\\cp.ckpt\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.0684 - acc: 0.1896 - val_loss: 2.0667 - val_acc: 0.1929\n",
      "Epoch 4/10\n",
      " 984/1000 [============================>.] - ETA: 0s - loss: 2.0642 - acc: 0.1999\n",
      "Epoch 00004: saving model to training_1\\cp.ckpt\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.0642 - acc: 0.2002 - val_loss: 2.0621 - val_acc: 0.2070\n",
      "Epoch 5/10\n",
      " 993/1000 [============================>.] - ETA: 0s - loss: 2.0600 - acc: 0.2032\n",
      "Epoch 00005: saving model to training_1\\cp.ckpt\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 2.0600 - acc: 0.2034 - val_loss: 2.0580 - val_acc: 0.2107\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.0561 - acc: 0.2147\n",
      "Epoch 00006: saving model to training_1\\cp.ckpt\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 2.0561 - acc: 0.2147 - val_loss: 2.0541 - val_acc: 0.2106\n",
      "Epoch 7/10\n",
      " 995/1000 [============================>.] - ETA: 0s - loss: 2.0525 - acc: 0.2126\n",
      "Epoch 00007: saving model to training_1\\cp.ckpt\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 2.0525 - acc: 0.2125 - val_loss: 2.0509 - val_acc: 0.2087\n",
      "Epoch 8/10\n",
      " 993/1000 [============================>.] - ETA: 0s - loss: 2.0491 - acc: 0.2115\n",
      "Epoch 00008: saving model to training_1\\cp.ckpt\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 2.0491 - acc: 0.2113 - val_loss: 2.0475 - val_acc: 0.2167\n",
      "Epoch 9/10\n",
      " 996/1000 [============================>.] - ETA: 0s - loss: 2.0458 - acc: 0.2177\n",
      "Epoch 00009: saving model to training_1\\cp.ckpt\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 2.0458 - acc: 0.2177 - val_loss: 2.0444 - val_acc: 0.2132\n",
      "Epoch 10/10\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 2.0427 - acc: 0.2180\n",
      "Epoch 00010: saving model to training_1\\cp.ckpt\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 2.0427 - acc: 0.2179 - val_loss: 2.0413 - val_acc: 0.2140\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    feature_extractor = FeatureExtractor(trainable=False)\n",
    "    classifier = Classifier(n_classes=8)\n",
    "    model = SampleModel(feature_extractor,classifier,is_online = True)\n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['acc'])\n",
    "\n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                  save_weights_only=True,\n",
    "                                                  verbose=1)\n",
    "    # Train the model\n",
    "    history = model.fit(train_images, train_labels, epochs=10, \n",
    "                      validation_data=(test_images, test_labels),callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XYzlJjHBpA8"
   },
   "source": [
    "### Pretrained Feature extractor model (ResNet) and Custom classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oda2-7cP1zuc"
   },
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "QgCCKOG012Dz",
    "outputId": "09b28d8c-6f39-48be-fa58-8a7092138d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images after filtering labels (32000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "old_labels = [0,1,2,3,4,5,6,7]\n",
    "new_labels = [] # remaining labels are 8 and 9\n",
    "allowed_labels = old_labels + new_labels\n",
    "train_images,train_labels,test_images,test_labels = custom_dataset_loader(dataset = dataset, \n",
    "                                                                          allowed_labels = allowed_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FYt0tfJ1-Yy"
   },
   "source": [
    "#### Check points and callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3GB98bI2Ayq"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"training_2/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-LoRGfECA2b"
   },
   "outputs": [],
   "source": [
    "class Classifier(Model):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.n_classes = n_classes\n",
    "        self.d1 = Dense(64, activation='relu')\n",
    "        self.d2 = Dense(n_classes)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "L6cqn7t0D_Zc",
    "outputId": "c6423403-f2ff-4596-b387-95dba3ea0691"
   },
   "outputs": [],
   "source": [
    "# This custom classifier is assumed to be pretrained. A new classifier layer will be created and will be set to trainable\n",
    "# while instantiating SampleModel class\n",
    "feature_extractor = tf.keras.applications.ResNet50()\n",
    "classifier = Classifier(n_classes = 8)\n",
    "model = SampleModel(feature_extractor,classifier,is_online=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "Mg5bZEXTETgx",
    "outputId": "feddc64e-8423-4f79-c373-d22ef7e63933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_1:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 32, 32, 3).\n",
      "Model: \"sample_model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 1000)              25636712  \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 8)                 64656     \n",
      "=================================================================\n",
      "Total params: 25,701,368\n",
      "Trainable params: 72\n",
      "Non-trainable params: 25,701,296\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build((None,32,32,3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIRxOGvVEokp"
   },
   "source": [
    "#### Train only the classifier\n",
    "Note: There is a part of classifier already trained at previous run;\n",
    "Therefore, by adding new classifier inside sample model instance, will set the previous sequential layer of classifier False. Only the new instance of the layer appended to the classifier will be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "LO-zyeItEYPv",
    "outputId": "58b3d3c7-6fb3-48ca-fc2f-6ecd575e840c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_2:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_2:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.0796 - acc: 0.1192WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_2:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "\n",
      "Epoch 00001: saving model to training_2\\cp.ckpt\n",
      "1000/1000 [==============================] - 159s 159ms/step - loss: 2.0796 - acc: 0.1192 - val_loss: 2.0787 - val_acc: 0.1264\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.0778 - acc: 0.1349\n",
      "Epoch 00002: saving model to training_2\\cp.ckpt\n",
      "1000/1000 [==============================] - 157s 157ms/step - loss: 2.0778 - acc: 0.1349 - val_loss: 2.0773 - val_acc: 0.1281\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.0763 - acc: 0.1500\n",
      "Epoch 00003: saving model to training_2\\cp.ckpt\n",
      "1000/1000 [==============================] - 160s 160ms/step - loss: 2.0763 - acc: 0.1500 - val_loss: 2.0756 - val_acc: 0.1510\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.0748 - acc: 0.1604\n",
      "Epoch 00004: saving model to training_2\\cp.ckpt\n",
      "1000/1000 [==============================] - 164s 164ms/step - loss: 2.0748 - acc: 0.1604 - val_loss: 2.0740 - val_acc: 0.1873\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.0733 - acc: 0.1773\n",
      "Epoch 00005: saving model to training_2\\cp.ckpt\n",
      "1000/1000 [==============================] - 167s 167ms/step - loss: 2.0733 - acc: 0.1773 - val_loss: 2.0725 - val_acc: 0.1784\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.0719 - acc: 0.1716\n",
      "Epoch 00006: saving model to training_2\\cp.ckpt\n",
      "1000/1000 [==============================] - 165s 165ms/step - loss: 2.0719 - acc: 0.1716 - val_loss: 2.0712 - val_acc: 0.1737\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.0705 - acc: 0.1815\n",
      "Epoch 00007: saving model to training_2\\cp.ckpt\n",
      "1000/1000 [==============================] - 168s 168ms/step - loss: 2.0705 - acc: 0.1815 - val_loss: 2.0699 - val_acc: 0.1971\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.0692 - acc: 0.1870\n",
      "Epoch 00008: saving model to training_2\\cp.ckpt\n",
      "1000/1000 [==============================] - 166s 166ms/step - loss: 2.0692 - acc: 0.1870 - val_loss: 2.0686 - val_acc: 0.1945\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.0678 - acc: 0.1891\n",
      "Epoch 00009: saving model to training_2\\cp.ckpt\n",
      "1000/1000 [==============================] - 168s 168ms/step - loss: 2.0678 - acc: 0.1891 - val_loss: 2.0675 - val_acc: 0.1776\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.0666 - acc: 0.1818\n",
      "Epoch 00010: saving model to training_2\\cp.ckpt\n",
      "1000/1000 [==============================] - 169s 169ms/step - loss: 2.0666 - acc: 0.1818 - val_loss: 2.0660 - val_acc: 0.1816\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "\n",
    "    feature_extractor = tf.keras.applications.ResNet50()\n",
    "    classifier = Classifier(n_classes=8)\n",
    "    model = SampleModel(feature_extractor,classifier,is_online = True)\n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['acc'])\n",
    "\n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                  save_weights_only=True,\n",
    "                                                  verbose=1)\n",
    "    # Train the model\n",
    "    history = model.fit(train_images, train_labels, epochs=10, \n",
    "                      validation_data=(test_images, test_labels),callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QbFw70SiAU9B"
   },
   "source": [
    "### Auto tune wrapper with TF callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySJHDjI5NvX8"
   },
   "source": [
    "#### Sample model with Pretrained Resnet as feature extractor and new custom classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "53N6xAVIOIM7"
   },
   "outputs": [],
   "source": [
    "class Classifier(Model):\n",
    "    def __init__(self, n_units, n_layers, dropout_rate, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.n_units = n_units\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.model= tf.keras.models.Sequential()\n",
    "        for layer in range(n_layers):\n",
    "            self.model.add(tf.keras.layers.Dense(self.n_units, activation='relu', \n",
    "                                                 kernel_initializer='glorot_normal',\n",
    "                                                 kernel_regularizer='l2'))\n",
    "            self.model.add(tf.keras.layers.Dropout(self.dropout_rate))\n",
    "\n",
    "        self.model.add(tf.keras.layers.Dense(self.n_classes, activation = tf.nn.softmax,\n",
    "                                             kernel_initializer='glorot_normal', \n",
    "                                             kernel_regularizer='l2'))\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class SampleModel(Model):\n",
    "    def __init__(self, feature_extractor,classifier):\n",
    "        super(SampleModel, self).__init__()\n",
    "        self.feature_extractor=feature_extractor\n",
    "        self.feature_extractor.trainable = False\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def call(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        x = self.classifier(features)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DNvZB69dDZxE"
   },
   "source": [
    "### Tensorboard to visualize the performance of the network with combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOxR5gnUmGlr"
   },
   "outputs": [],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93c0_qpY5TwI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "# GPU Distributed training with Mirror Strategy\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete combination of all hyperparameters; Random search is another option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Pe5gm1pB5KU"
   },
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('HP_NUM_UNITS', hp.Discrete([8,64]))\n",
    "HP_NUM_LAYERS = hp.HParam('HP_NUM_LAYERS',hp.Discrete([1,5]))\n",
    "HP_OPTIMIZER = hp.HParam('HP_OPTIMIZER', hp.Discrete(['adam', 'nadam']))\n",
    "# HP_OPTIMIZER = hp.HParam('HP_OPTIMIZER', hp.Discrete(['adam']))\n",
    "HP_DROPOUT = hp.HParam('HP_DROPOUT', hp.RealInterval(0.1,0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8QcRj-xCHYu"
   },
   "outputs": [],
   "source": [
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_NUM_UNITS, HP_NUM_LAYERS, HP_DROPOUT, HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt tensorflow runs to log hyperparameters and metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-ePLITdEMMG"
   },
   "outputs": [],
   "source": [
    "def train_test_model(run_dir, hparams):\n",
    "    with strategy.scope():\n",
    "        feature_extractor = tf.keras.applications.ResNet50()\n",
    "        classifier = Classifier(hparams[HP_NUM_UNITS], \n",
    "                                hparams[HP_NUM_LAYERS],\n",
    "                                hparams[HP_DROPOUT],\n",
    "                                n_classes=8)  # Change the number of classes\n",
    "        model = SampleModel(feature_extractor,classifier)\n",
    "        \n",
    "        model.compile(optimizer=hparams[HP_OPTIMIZER],\n",
    "                        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                        save_weights_only=True,\n",
    "                                                        verbose=1)\n",
    "        \n",
    "        hparams_callback = tf.keras.callbacks.TensorBoard(run_dir + \"/keras\")\n",
    "\n",
    "        history = model.fit(train_images, train_labels, epochs=3) # Just 3 epochss\n",
    "        _, accuracy = model.evaluate(test_images,test_labels)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wJ9ZHPpFvDJ"
   },
   "source": [
    "For each run, log an hparams summary with the hyperparameters and final accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aRR-3B2AFtJL"
   },
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = train_test_model(run_dir,hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6g90rfLaF0MU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'HP_NUM_LAYERS': 1, 'HP_NUM_UNITS': 8, 'HP_DROPOUT': 0.1, 'HP_OPTIMIZER': 'adam'}\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_13:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_13:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 2.1002 - accuracy: 0.1238\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 2.0795 - accuracy: 0.1232\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 124s 124ms/step - loss: 2.0795 - accuracy: 0.1257\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_13:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "250/250 [==============================] - 31s 124ms/step - loss: 2.0795 - accuracy: 0.1195\n",
      "--- Starting trial: run-1\n",
      "{'HP_NUM_LAYERS': 1, 'HP_NUM_UNITS': 8, 'HP_DROPOUT': 0.1, 'HP_OPTIMIZER': 'nadam'}\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_14:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_14:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 2.1046 - accuracy: 0.1213\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 132s 132ms/step - loss: 2.0795 - accuracy: 0.1218\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 135s 135ms/step - loss: 2.0795 - accuracy: 0.1264\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_14:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "250/250 [==============================] - 33s 134ms/step - loss: 2.0795 - accuracy: 0.1195\n",
      "--- Starting trial: run-2\n",
      "{'HP_NUM_LAYERS': 1, 'HP_NUM_UNITS': 8, 'HP_DROPOUT': 0.2, 'HP_OPTIMIZER': 'adam'}\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_15:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_15:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "1000/1000 [==============================] - 135s 135ms/step - loss: 2.1040 - accuracy: 0.1249\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 135s 135ms/step - loss: 2.0795 - accuracy: 0.1248\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 135s 135ms/step - loss: 2.0795 - accuracy: 0.1264\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_15:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 2.0795 - accuracy: 0.1195\n",
      "--- Starting trial: run-3\n",
      "{'HP_NUM_LAYERS': 1, 'HP_NUM_UNITS': 8, 'HP_DROPOUT': 0.2, 'HP_OPTIMIZER': 'nadam'}\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_16:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_16:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "1000/1000 [==============================] - 136s 136ms/step - loss: 2.1004 - accuracy: 0.1245\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 137s 137ms/step - loss: 2.0795 - accuracy: 0.1237\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 135s 135ms/step - loss: 2.0795 - accuracy: 0.1264\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_16:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 2.0795 - accuracy: 0.1195\n",
      "--- Starting trial: run-4\n",
      "{'HP_NUM_LAYERS': 1, 'HP_NUM_UNITS': 64, 'HP_DROPOUT': 0.1, 'HP_OPTIMIZER': 'adam'}\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_17:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_17:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "1000/1000 [==============================] - 135s 135ms/step - loss: 2.1227 - accuracy: 0.1259\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 134s 134ms/step - loss: 2.0795 - accuracy: 0.1253\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 135s 135ms/step - loss: 2.0795 - accuracy: 0.1228\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_17:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 2.0795 - accuracy: 0.1195\n",
      "--- Starting trial: run-5\n",
      "{'HP_NUM_LAYERS': 1, 'HP_NUM_UNITS': 64, 'HP_DROPOUT': 0.1, 'HP_OPTIMIZER': 'nadam'}\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_18:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_18:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "1000/1000 [==============================] - 135s 135ms/step - loss: 2.1252 - accuracy: 0.1244\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 136s 136ms/step - loss: 2.0795 - accuracy: 0.1242\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 136s 136ms/step - loss: 2.0795 - accuracy: 0.1248\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_18:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 2.0795 - accuracy: 0.1195\n",
      "--- Starting trial: run-6\n",
      "{'HP_NUM_LAYERS': 1, 'HP_NUM_UNITS': 64, 'HP_DROPOUT': 0.2, 'HP_OPTIMIZER': 'adam'}\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_19:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_19:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "1000/1000 [==============================] - 135s 135ms/step - loss: 2.1231 - accuracy: 0.1233\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 135s 135ms/step - loss: 2.0795 - accuracy: 0.1253\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 135s 135ms/step - loss: 2.0795 - accuracy: 0.1262\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_19:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "250/250 [==============================] - 33s 134ms/step - loss: 2.0795 - accuracy: 0.1195\n",
      "--- Starting trial: run-7\n",
      "{'HP_NUM_LAYERS': 1, 'HP_NUM_UNITS': 64, 'HP_DROPOUT': 0.2, 'HP_OPTIMIZER': 'nadam'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_20:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_20:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "1000/1000 [==============================] - 136s 136ms/step - loss: 2.1262 - accuracy: 0.1221\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 136s 136ms/step - loss: 2.0795 - accuracy: 0.1244\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 135s 135ms/step - loss: 2.0795 - accuracy: 0.1252\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_20:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 2.0795 - accuracy: 0.1195\n",
      "--- Starting trial: run-8\n",
      "{'HP_NUM_LAYERS': 5, 'HP_NUM_UNITS': 8, 'HP_DROPOUT': 0.1, 'HP_OPTIMIZER': 'adam'}\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_21:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_21:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "1000/1000 [==============================] - 135s 135ms/step - loss: 2.1534 - accuracy: 0.1215\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 136s 136ms/step - loss: 2.0797 - accuracy: 0.1264\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 136s 136ms/step - loss: 2.0795 - accuracy: 0.1264\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_21:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 2.0795 - accuracy: 0.1195\n",
      "--- Starting trial: run-9\n",
      "{'HP_NUM_LAYERS': 5, 'HP_NUM_UNITS': 8, 'HP_DROPOUT': 0.1, 'HP_OPTIMIZER': 'nadam'}\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_22:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_22:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "1000/1000 [==============================] - 142s 142ms/step - loss: 2.1663 - accuracy: 0.1242\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 142s 142ms/step - loss: 2.0797 - accuracy: 0.1229\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 141s 141ms/step - loss: 2.0795 - accuracy: 0.1244\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_22:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 2.0795 - accuracy: 0.1195\n",
      "--- Starting trial: run-10\n",
      "{'HP_NUM_LAYERS': 5, 'HP_NUM_UNITS': 8, 'HP_DROPOUT': 0.2, 'HP_OPTIMIZER': 'adam'}\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_23:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_23:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "1000/1000 [==============================] - 136s 136ms/step - loss: 2.1519 - accuracy: 0.1246\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 136s 136ms/step - loss: 2.0797 - accuracy: 0.1243\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 136s 136ms/step - loss: 2.0795 - accuracy: 0.1264\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_23:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (32, 32, 32, 3).\n",
      "247/250 [============================>.] - ETA: 0s - loss: 2.0795 - accuracy: 0.1197"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "for num_layers in HP_NUM_LAYERS.domain.values:\n",
    "    for num_units in HP_NUM_UNITS.domain.values:\n",
    "        for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "            for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                hparams = {\n",
    "                    HP_NUM_LAYERS:num_layers,\n",
    "                    HP_NUM_UNITS: num_units,\n",
    "                    HP_DROPOUT: dropout_rate,\n",
    "                    HP_OPTIMIZER: optimizer,\n",
    "                }\n",
    "                run_name = \"run-%d\" % session_num\n",
    "                print('--- Starting trial: %s' % run_name)\n",
    "                print({h.name: hparams[h] for h in hparams})\n",
    "                run('logs/hparam_tuning/' + run_name, hparams)\n",
    "                session_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VtPMprJhyXnb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1260), started 2:35:12 ago. (Use '!kill 1260' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ef585339fb99194c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ef585339fb99194c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "auto_tune_incremental_learning_model_wrapper.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
